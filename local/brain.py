from pydantic import BaseModel
from openai import OpenAI
import traceback
import os
import time
import base64
import json
import glob
import uuid
from datetime import datetime
from generate_audio import t2a_minimax

from local_tracer import get_tracer

tracer = get_tracer("./cache/logs")

HOST_URL = "https://helped-monthly-alpaca.ngrok-free.app"

class HtmlView(BaseModel):
    height: int
    width: int
    html: str
    danmu_text: str

class isFinished(BaseModel):
    result: bool

def call_openai_api(messages, thread_id=None, response_format=HtmlView, model="gpt-4o-mini", max_tokens=2000):
    """
    è°ƒç”¨OpenAI APIè¿›è¡Œå›¾åƒå’Œè¯­éŸ³åˆ†æ
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        thread_id: çº¿ç¨‹ID
        response_format: è¿”å›æ ¼å¼
        model: ä½¿ç”¨çš„æ¨¡å‹
        max_tokens: æœ€å¤§tokenæ•°ï¼Œé»˜è®¤2000
    Returns:
        (response, err): å“åº”å¯¹è±¡å’Œé”™è¯¯ï¼ˆå¦‚æœ‰ï¼‰
    """
    if thread_id is None:
        thread_id = f"local_thread_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
    
    client = OpenAI(timeout=30.0)
    try:
        if response_format == "text":
            completion = client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens
            )
            response = completion.choices[0].message.content
        else:
            completion = client.beta.chat.completions.parse(
                model=model,
                messages=messages,
                response_format=response_format,
                max_tokens=max_tokens
            )
            response = completion.choices[0].message.parsed
        tracer.log_brain_conversation(
            thread_id=thread_id,
            messages=messages,
            result=response
        )
        return response, None
    except Exception as e:
        print(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
        if response_format == "text":
            return "", e
        return HtmlView(
            height=400,
            width=600,
            html="<div style='padding: 16px; background-color: white; border-radius: 16px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); color: #007aff;'><h2 style='font-size: 20px; font-weight: bold; margin-bottom: 8px;'>ç³»ç»Ÿæç¤º</h2><p style='margin-bottom: 16px;'>ç½‘ç»œè¿æ¥è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚</p></div>",
            danmu_text="ç½‘ç»œè¿æ¥è¶…æ—¶"
        ), e

def update_status_json(fields: dict):
    status_path = "cache/status.json"
    if os.path.exists(status_path):
        try:
            with open(status_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception as e:
            print(f"è¯»å–status.jsonå¤±è´¥: {e}")
            data = {}
    else:
        data = {}
    data.update(fields)
    print("update_status_json:", data)
    with open(status_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def reset_status():
    data = {
        "action": "render",
        "value": "",
        "voice": "https://helped-monthly-alpaca.ngrok-free.app/voice/hello.mp3",
        "timestamp": int(time.time()),
        "html": """
    <style>
      /* ================= 1. å…¨å±€ ================= */ 
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        font-family: -apple-system, BlinkMacSystemFont, "PingFang SC",
          sans-serif;
      }
      body {
        height: 100vh;
        display: flex;
        align-items: center;
        justify-content: center;
        background: #0e0e14;
        overflow: hidden;
      }

      /* ================= 2. low-poly èƒŒæ™¯ ================= */
      #bg-canvas {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        z-index: 0;
        opacity: 0.8;
      }
      #bg-canvas canvas {
        transform: scale(1.05);
      }

      /* ================= 3. ä¸»å¡ç‰‡ ================= */
      .card-ani {
        width: 480px;
        height: 380px;
        background: linear-gradient(135deg, #69bff9, #b96af3, #e9685e, #f2ac3e);
        border-radius: 18px;
        padding: 4px;
        display: flex;
        flex-direction: column;
        justify-content: space-between;
        border: 1px solid #fff4;
        box-shadow: 0 4px 24px 0 #b96af366;
        animation: cardFadeIn 0.8s cubic-bezier(0.4, 2, 0.6, 1) 1,
          float 4s ease-in-out infinite 0.8s;
        position: relative;
        z-index: 2;
      }
      .card-inner {
        background: rgba(255, 255, 255, 0.95);
        border-radius: 14px;
        padding: 6px;
        flex: 1;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        text-align: center;
        animation: innerPop 0.7s cubic-bezier(0.4, 2, 0.6, 1) 1;
      }

      /* ================= 4. æ–‡å­— ================= */
      .title {
        font-size: 32px;
        font-weight: 800;
        color: #333;
        margin-bottom: 12px;
        position: relative;
        overflow: hidden;
        height: 40px;
        white-space: nowrap;
        display: flex;
        align-items: center;
        justify-content: center;
        opacity: 0;
        transform: translateY(-20px);
        animation: titleFlyIn 1s ease-out 0.5s forwards;
      }
      .desc {
        font-size: 18px;
        color: #555;
        line-height: 1.6;
        max-width: 380px;
        min-height: 60px;
        display: flex;
        align-items: center;
        justify-content: center;
        opacity: 0;
        transform: translateX(-30px);
        animation: descSlideIn 1s ease-out 1s forwards;
      }
      .sub-desc {
        font-size: 16px;
        color: #777;
        line-height: 1.4;
        max-width: 380px;
        min-height: 30px;
        display: flex;
        align-items: center;
        justify-content: center;
        opacity: 0;
        transform: translateX(30px);
        animation: descSlideIn 1s ease-out 1.5s forwards;
        margin-top: 8px;
      }
      /* çˆ†ç‚¸ç²’å­ */
      #explode {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        pointer-events: none;
        z-index: 3;
      }
      /* ================= 5. æŒ‰é’® ================= */
      .ani-btn {
        background-size: 400% 100%;
        background-image: linear-gradient(
          45deg,
          #69bff9,
          #b96af3,
          #e9685e,
          #f2ac3e,
          #69bff9
        );
        color: #fff;
        border: none;
        border-radius: 8px;
        padding: 10px 28px;
        font-size: 16px;
        cursor: pointer;
        margin-top: 20px;
        transition: transform 0.25s cubic-bezier(0.4, 2, 0.6, 1),
          box-shadow 0.25s;
        box-shadow: 0 2px 8px 0 #b96af377;
        animation: rainbow 4s ease-in-out infinite;
      }
      .ani-btn:hover {
        transform: scale(1.12) translateY(-3px);
        box-shadow: 0 6px 18px 0 #b96af3aa;
      }

      /* ================= 6. åŠ¨ç”»å®šä¹‰ ================= */
      @keyframes cardFadeIn {
        0% {
          opacity: 0;
          transform: scale(0.9) translateY(40px);
        }
        100% {
          opacity: 1;
          transform: scale(1) translateY(0);
        }
      }
      @keyframes innerPop {
        0% {
          opacity: 0;
          transform: scale(0.8);
        }
        100% {
          opacity: 1;
          transform: scale(1);
        }
      }
      @keyframes float {
        0%,
        100% {
          transform: translateY(0);
        }
        50% {
          transform: translateY(-8px);
        }
      }
      @keyframes rainbow {
        0% {
          background-position: 0% 50%;
        }
        50% {
          background-position: 100% 50%;
        }
        100% {
          background-position: 0% 50%;
        }
      }

      /* æ–°å¢æ–‡å­—åŠ¨ç”» */
      @keyframes titleFlyIn {
        0% {
          opacity: 0;
          transform: translateY(-20px) scale(0.8);
        }
        50% {
          opacity: 0.7;
          transform: translateY(-5px) scale(1.1);
        }
        100% {
          opacity: 1;
          transform: translateY(0) scale(1);
        }
      }

      @keyframes descSlideIn {
        0% {
          opacity: 0;
          transform: translateX(-30px);
        }
        100% {
          opacity: 1;
          transform: translateX(0);
        }
      }

      @keyframes textFadeIn {
        0% {
          opacity: 0;
          transform: translateY(10px);
        }
        100% {
          opacity: 1;
          transform: translateY(0);
        }
      }

      @keyframes textBounce {
        0% {
          opacity: 0;
          transform: scale(0.3);
        }
        50% {
          opacity: 1;
          transform: scale(1.05);
        }
        70% {
          transform: scale(0.9);
        }
        100% {
          opacity: 1;
          transform: scale(1);
        }
      }

      @keyframes textGlow {
        0%,
        100% {
          text-shadow: 0 0 5px rgba(105, 191, 249, 0.5);
        }
        50% {
          text-shadow: 0 0 20px rgba(105, 191, 249, 0.8),
            0 0 30px rgba(105, 191, 249, 0.6);
        }
      }

      /* æ‰“å­—æœºå…‰æ ‡ */
      .typing-cursor::after {
        content: "|";
        animation: blink 1s infinite;
        color: #69bff9;
        font-weight: bold;
      }

      @keyframes blink {
        0%,
        50% {
          opacity: 1;
        }
        51%,
        100% {
          opacity: 0;
        }
      }
    </style>
    <body>
    <!-- low-poly èƒŒæ™¯ -->
    <div id="bg-canvas"></div>

    <!-- ä¸»å¡ç‰‡ -->
    <div class="card-ani">
      <div class="card-inner">
        <div class="title" id="title">NoNoMi äººç”Ÿæ»¤é•œ</div>
        <div class="desc" id="desc">æ¿€å‘åˆ›é€  â€¢ ä¸°å¯Œç”Ÿæ´»</div>
        <div class="sub-desc" id="sub-desc">æ„å»ºAGIæ£®æ—ğŸŒ³</div>
        <a
          href="https://www.xiaohongshu.com/user/profile/620103f00000000021029b87?xsec_token=YBwTrdN0RlEzIjqWDoW7NrR9KQeXLfFH4_64sZWEgYH1g=&xsec_source=app_share&xhsshare=CopyLink&appuid=620103f00000000021029b87&apptime=1753543002&share_id=4ed639c86e0a451dad4f0e5a53ea7688"
          target="_blank"
          class="ani-btn"
          id="btn"
          >ç‚¹å‡»Linkä¸»åˆ›</a
        >
      </div>
    </div>

    <!-- çˆ†ç‚¸ç²’å­å±‚ -->
    <canvas id="explode"></canvas>

    <!-- ================= è„šæœ¬ ================= -->
    <script src="https://cdn.jsdelivr.net/npm/three@0.160.0/build/three.min.js  "></script>
    <script>
      /* ---------- 1. low-poly èƒŒæ™¯ ---------- */
      const scene = new THREE.Scene();
      const camera = new THREE.PerspectiveCamera(
        75,
        innerWidth / innerHeight,
        0.1,
        1000
      );
      camera.position.z = 50;

      const renderer = new THREE.WebGLRenderer({
        canvas: document.createElement("canvas"),
        alpha: true,
      });
      renderer.setSize(innerWidth, innerHeight);
      document.getElementById("bg-canvas").appendChild(renderer.domElement);

      /* éšæœºé¢œè‰²å‡½æ•° */
      const randomColor = () =>
        new THREE.Color(Math.random(), Math.random(), Math.random());

      /* åˆ›å»º low-poly ç½‘æ ¼ */
      function createMesh() {
        const geo = new THREE.IcosahedronGeometry(26, 1);
        const mat = new THREE.MeshBasicMaterial({
          color: randomColor(),
          wireframe: true,
          transparent: true,
          opacity: 0.25,
        });
        const mesh = new THREE.Mesh(geo, mat);
        scene.add(mesh);
        return mesh;
      }
      const mesh1 = createMesh();
      const mesh2 = createMesh();
      mesh2.rotation.set(
        Math.random() * Math.PI,
        Math.random() * Math.PI,
        Math.random() * Math.PI
      );

      /* åŠ¨ç”»å¾ªç¯ */
      function animate() {
        requestAnimationFrame(animate);
        mesh1.rotation.x += 0.0012;
        mesh1.rotation.y += 0.0015;
        mesh2.rotation.x -= 0.0013;
        mesh2.rotation.y -= 0.0017;
        renderer.render(scene, camera);
      }
      animate();

      /* ---------- 2. æµå¼æ–‡å­—æ›´æ–° ---------- */
      function updateText(elementId, newText, animationType = "fadeIn") {
        const element = document.getElementById(elementId);
        if (element) {
          // ç§»é™¤ä¹‹å‰çš„åŠ¨ç”»ç±»
          element.classList.remove("typing-cursor", "text-glow", "text-bounce");

          // æ ¹æ®åŠ¨ç”»ç±»å‹åº”ç”¨ä¸åŒçš„æ•ˆæœ
          switch (animationType) {
            case "typewriter":
              typewriterEffect(element, newText);
              break;
            case "bounce":
              bounceEffect(element, newText);
              break;
            case "glow":
              glowEffect(element, newText);
              break;
            case "fadeIn":
            default:
              fadeInEffect(element, newText);
              break;
          }
        }
      }

      // æ‰“å­—æœºæ•ˆæœ
      function typewriterEffect(element, text) {
        element.textContent = "";
        element.classList.add("typing-cursor");
        let index = 0;

        function type() {
          if (index < text.length) {
            element.textContent += text.charAt(index);
            index++;
            setTimeout(type, 100);
          } else {
            element.classList.remove("typing-cursor");
          }
        }
        type();
      }

      // å¼¹è·³æ•ˆæœ
      function bounceEffect(element, text) {
        element.style.animation = "none";
        element.offsetHeight; // è§¦å‘é‡æ’
        element.innerHTML = text;
        element.style.animation = "textBounce 0.8s ease-out";
      }

      // å‘å…‰æ•ˆæœ
      function glowEffect(element, text) {
        element.innerHTML = text;
        element.classList.add("text-glow");
        setTimeout(() => {
          element.classList.remove("text-glow");
        }, 2000);
      }

      // æ·¡å…¥æ•ˆæœ
      function fadeInEffect(element, text) {
        element.style.animation = "none";
        element.offsetHeight; // è§¦å‘é‡æ’
        element.innerHTML = text;
        element.style.animation = "textFadeIn 0.6s ease-out";
      }

      // åªä¿ç•™titleçš„åŠ¨ç”»æ•ˆæœ
      setTimeout(() => {
        updateText("title", "NoNoMi", "glow");
      }, 12000);

      /* ---------- 3. æŒ‰é’®ç‚¹å‡»ç²’å­çˆ†ç‚¸ ---------- */
      const explode = document.getElementById("explode");
      const ctx = explode.getContext("2d");
      explode.width = innerWidth;
      explode.height = innerHeight;

      let particles = [];
      class Particle {
        constructor(x, y) {
          this.x = x;
          this.y = y;
          this.vx = (Math.random() - 0.5) * 8;
          this.vy = (Math.random() - 0.5) * 8;
          this.life = 60;
          this.color = `hsl(${Math.random() * 360},100%,70%)`;
        }
        update() {
          this.x += this.vx;
          this.y += this.vy;
          this.life--;
        }
        draw() {
          ctx.globalAlpha = this.life / 60;
          ctx.fillStyle = this.color;
          ctx.beginPath();
          ctx.arc(this.x, this.y, 2, 0, Math.PI * 2);
          ctx.fill();
        }
      }

      document.getElementById("btn").addEventListener("click", (e) => {
        const rect = e.target.getBoundingClientRect();
        const x = rect.left + rect.width / 2;
        const y = rect.top + rect.height / 2;
        for (let i = 0; i < 60; i++) particles.push(new Particle(x, y));

        // ç‚¹å‡»æŒ‰é’®æ—¶åªè§¦å‘ç²’å­æ•ˆæœï¼Œæ–‡å­—ä¿æŒä¸å˜
      });

      function renderExplode() {
        ctx.clearRect(0, 0, innerWidth, innerHeight);
        particles.forEach((p, i) => {
          p.update();
          p.draw();
          if (p.life <= 0) particles.splice(i, 1);
        });
        requestAnimationFrame(renderExplode);
      }
      renderExplode();

      /* ---------- 4. çª—å£è‡ªé€‚åº” ---------- */
      addEventListener("resize", () => {
        camera.aspect = innerWidth / innerHeight;
        camera.updateProjectionMatrix();
        renderer.setSize(innerWidth, innerHeight);
        explode.width = innerWidth;
        explode.height = innerHeight;
             });
     </script>
 """,
        "danmu_text": "QAQ",
        "height": 400,
        "width": 600
    }
    update_status_json(data)

def periodic_ai_task():
    reset_status()
    time.sleep(2)
    print("å¼€å§‹AIä»»åŠ¡")
    AI_TIME_INTERVAL = int(os.environ.get("AI_TIME_INTERVAL", 5))
    SCREENSHOT_UPLOAD_AMOUNT = int(os.environ.get("SCREENSHOT_UPLOAD_AMOUNT", 1))
    USE_CAMERA = int(os.environ.get("USE_CAMERA", 0))
    USE_IMAGES = int(os.environ.get("USE_IMAGES", 0))  # æ–°å¢å›¾ç‰‡å¼€å…³ï¼Œé»˜è®¤0
    if USE_CAMERA == 1:
        IMAGE_DIR = os.path.join(".", "cache", "camera")
    else:
        IMAGE_DIR = os.path.join(".", "cache", "screenshot")
    AUDIO_TXT_PATH = os.path.join(".", "cache", "audio", "audio.txt")
    PROMPT_PATH = os.path.join(".", "prompt.txt")
    PROMPT_IMAGE_PATH = os.path.join(".", "prompt_image.txt")
    PROMPT_FINISHED_PATH = os.path.join(".", "prompt_finished.txt")

    while True:
        current_timestamp = int(time.time())

        # æ­¥éª¤1: æ£€æŸ¥éŸ³é¢‘å†…å®¹å¹¶åˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘AIå›å¤
        audio_content = None
        if os.path.exists(AUDIO_TXT_PATH):
            try:
                with open(AUDIO_TXT_PATH, "r", encoding="utf-8") as f:
                    audio_content = f.read().strip()
            except Exception as e:
                print(f"è¯»å–audio.txtå¤±è´¥: {e}")

        if not audio_content:
            print("æœªæ£€æµ‹åˆ°éŸ³é¢‘å†…å®¹ï¼Œ5ç§’åé‡è¯•ã€‚")
            time.sleep(5)
            continue

        # ä½¿ç”¨isFinishedæ¨¡å‹åˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘AIå›å¤
        try:
            with open(PROMPT_FINISHED_PATH, "r", encoding="utf-8") as f:
                finished_prompt = f.read().strip()
        except Exception as e:
            print(f"è¯»å–prompt_finished.txtå¤±è´¥: {e}")
            finished_prompt = ""

        if finished_prompt:
            finished_messages = [
                {"role": "system", "content": finished_prompt},
                {"role": "user", "content": audio_content}
            ]
            print("å¼€å§‹åˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘AIå›å¤", audio_content)
            try:
                finished_result, finished_err = call_openai_api(
                    finished_messages,
                    response_format=isFinished,
                    model="gpt-4o-mini",
                    max_tokens=50  # isFinishedåªéœ€è¦è¿”å›å¸ƒå°”å€¼ï¼Œ50ä¸ªtokenè¶³å¤Ÿ
                )
                if finished_err is not None:
                    print("åˆ¤æ–­è¯·æ±‚å¼‚å¸¸:", finished_err)
                    time.sleep(AI_TIME_INTERVAL)
                    continue
                
                if not finished_result.result:
                    print("âŒéŸ³é¢‘å†…å®¹ä¸éœ€è¦è§¦å‘AIå›å¤ï¼Œè·³è¿‡æœ¬æ¬¡å¤„ç†")
                    # æ£€æŸ¥å¦‚æœaudio.txtçš„è¡Œæ•°å¤§äº5è¡Œï¼Œåˆ™æ¸…ç©º
                    if os.path.exists(AUDIO_TXT_PATH):
                        with open(AUDIO_TXT_PATH, "r", encoding="utf-8") as f:
                            lines = f.readlines()
                            if len(lines) > 5: # å¦‚æœè¡Œæ•°å¤§äº5è¡Œï¼Œåˆ™æ¸…ç©º
                                with open(AUDIO_TXT_PATH, "w", encoding="utf-8") as f:
                                    f.write("")
                    time.sleep(3)
                    continue
                print("âœ…éŸ³é¢‘å†…å®¹éœ€è¦è§¦å‘AIå›å¤ï¼Œç»§ç»­å¤„ç†")
                # æ¸…ç©ºéŸ³é¢‘æ–‡ä»¶
                try:
                    with open(AUDIO_TXT_PATH, "w", encoding="utf-8") as f:
                        f.write("")
                except Exception as e:
                    print(f"æ¸…ç©ºaudio.txtå¤±è´¥: {e}")
                time.sleep(AI_TIME_INTERVAL)
                update_status_json({
                    "action": "pending",
                    "voice": "https://helped-monthly-alpaca.ngrok-free.app/voice/pending.mp3",
                    "timestamp": int(time.time()),
                })
            except Exception as e:
                print(f"åˆ¤æ–­è¯·æ±‚å¼‚å¸¸: {e}")
                time.sleep(AI_TIME_INTERVAL)
                continue
        else:
            print("ç¼ºå°‘finished promptï¼Œè·³è¿‡åˆ¤æ–­æ­¥éª¤")

        # æ­¥éª¤2: å›¾ç‰‡åˆ†æå¼€å…³
        image_files = []
        latest_images = []
        image_messages = []
        prompt_image = ""
        screen_description = None
        image_analysis_error = False

        if USE_IMAGES == 1:
            # åªæœ‰USE_IMAGESä¸º1æ—¶æ‰è¿›è¡Œå›¾ç‰‡ç›¸å…³å¤„ç†
            image_files = glob.glob(os.path.join(IMAGE_DIR, "*"))
            image_files = [f for f in image_files if os.path.isfile(f)]
            image_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
            latest_images = image_files[:SCREENSHOT_UPLOAD_AMOUNT]

            if len(image_files) > 0:
                print(f"æ£€æµ‹åˆ° {len(image_files)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹å¤„ç†")
                for img_path in latest_images:
                    try:
                        with open(img_path, "rb") as f:
                            b64_img = base64.b64encode(f.read()).decode("utf-8")
                        image_messages.append({
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}
                        })
                    except Exception as e:
                        print(f"è¯»å–å›¾ç‰‡å¤±è´¥: {img_path}, {e}")
            else:
                print("æœªæ£€æµ‹åˆ°å›¾ç‰‡ï¼Œè·³è¿‡å›¾ç‰‡å¤„ç†æ­¥éª¤")

            try:
                with open(PROMPT_IMAGE_PATH, "r", encoding="utf-8") as f:
                    prompt_image = f.read().strip()
            except Exception as e:
                print(f"è¯»å–prompt_image.txtå¤±è´¥: {e}")
                prompt_image = ""

            t1 = time.time()
            if prompt_image and image_messages:
                image_analysis_messages = [
                    {"role": "system", "content": prompt_image},
                    {"role": "user", "content": image_messages},
                    {"role": "user", "content": f"audio: {audio_content}"}
                ]
                print("å¼€å§‹å›¾ç‰‡ç»†èŠ‚åˆ†æè¯·æ±‚", latest_images, audio_content)
                try:
                    response, err = call_openai_api(
                        image_analysis_messages,
                        response_format="text",
                        max_tokens=500  # å›¾ç‰‡åˆ†ææ–‡æœ¬ï¼Œé€šå¸¸500ä¸ªtokenè¶³å¤Ÿæè¿°å›¾ç‰‡
                    )
                    t2 = time.time()
                    print("âœ… å›¾ç‰‡ç»†èŠ‚åˆ†æç»“æœ:", response)
                    print(f"å›¾ç‰‡ç»†èŠ‚åˆ†æè€—æ—¶: {t2-t1:.2f}ç§’")
                    if err is not None:
                        print("å›¾ç‰‡ç»†èŠ‚åˆ†æè¯·æ±‚å¼‚å¸¸:", err)
                        image_analysis_error = True
                    elif isinstance(response, str):
                        screen_description = response
                    elif hasattr(response, "content"):
                        screen_description = getattr(response, "content", None)
                    else:
                        screen_description = str(response)
                except Exception as e:
                    t2 = time.time()
                    print(f"å›¾ç‰‡ç»†èŠ‚åˆ†æè¯·æ±‚å¼‚å¸¸: {e}")
                    print(f"å›¾ç‰‡ç»†èŠ‚åˆ†æè€—æ—¶: {t2-t1:.2f}ç§’")
                    image_analysis_error = True
            else:
                t2 = time.time()
                print("æœªèƒ½è¿›è¡Œå›¾ç‰‡ç»†èŠ‚åˆ†æï¼ˆç¼ºå°‘prompt_imageæˆ–å›¾ç‰‡ï¼‰")
                image_analysis_error = True
        else:
            print("USE_IMAGES=0ï¼Œè·³è¿‡å›¾ç‰‡åˆ†ææ­¥éª¤")
            image_analysis_error = True  # å¼ºåˆ¶ä¸åŠ å›¾ç‰‡åˆ†æç»“æœ

        # æ­¥éª¤3: ä¸»è¯·æ±‚
        try:
            with open(PROMPT_PATH, "r", encoding="utf-8") as f:
                system_prompt = f.read().strip()
        except Exception as e:
            print(f"è¯»å–prompt.txtå¤±è´¥: {e}")
            system_prompt = ""

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ éŸ³é¢‘å†…å®¹ï¼ˆæ ¼å¼åŒ–ä¸ºaudio:å‰ç¼€ï¼‰
        if audio_content:
            messages.append({"role": "user", "content": f"audio: {audio_content}"})
        
        # å¦‚æœæœ‰å›¾ç‰‡åˆ†æç»“æœï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
        if not image_analysis_error and screen_description:
            messages.append({"role": "user", "content": f"screen_description: {screen_description}"})
        else:
            if USE_IMAGES == 1:
                print("å›¾ç‰‡åˆ†æå¤±è´¥ï¼Œä¸ä¼ é€’å›¾ç‰‡ä¿¡æ¯ç»™ä¸»è¯·æ±‚ã€‚")
            else:
                print("å›¾ç‰‡åˆ†æè¢«å…³é—­ï¼Œä¸ä¼ é€’å›¾ç‰‡ä¿¡æ¯ç»™ä¸»è¯·æ±‚ã€‚")
        
        if not messages:
            print("æ²¡æœ‰å¯ç”¨çš„è¾“å…¥ï¼Œè·³è¿‡æœ¬æ¬¡è°ƒç”¨ã€‚")
            time.sleep(AI_TIME_INTERVAL)
            continue
        print("æœ¬æ¬¡å¤„ç†çš„å›¾ç‰‡è·¯å¾„:", latest_images)
        t3 = time.time()
        try:
            # ç”ŸæˆåŸºäºå½“å‰æ—¶é—´çš„thread_id
            current_thread_id = f"local_brain_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
            result, err = call_openai_api(messages, thread_id=current_thread_id, max_tokens=2000)  # HtmlRenderä¸»è¯·æ±‚ï¼Œéœ€è¦æ›´å¤štokenæ¥ç”ŸæˆHTMLå’Œå¼¹å¹•
            t4 = time.time()
            print(f"âœ… ä¸»è¯·æ±‚è€—æ—¶: {t4-t3:.2f}ç§’")
            print("âœ… AI HTMLç»“æœ:", result)
            print(f"Thread ID: {current_thread_id}")
            # tracer.log_brain_conversation(
            #     thread_id=current_thread_id,
            #     messages=messages,
            #     result=result,
            #     image_paths=latest_images,
            #     audio_content=f"audio: {audio_content}" if audio_content else None
            # )
            if hasattr(result, "danmu_text") and result.danmu_text != "":
                audio = t2a_minimax(result.danmu_text)
                with open(f"cache/voice/{current_timestamp}.mp3", "wb") as f:
                    f.write(audio)
                route = f"/voice/{current_timestamp}.mp3"
                print("minimaxç»“æœ:", route)
            else:
                route = ""

            data = {
                "voice": "" if route == "" else f"{HOST_URL}{route}",
                "timestamp": int(time.time()),
                "html": getattr(result, "html", ""),
                "danmu_text": getattr(result, "danmu_text", ""),
                "height": 600, # getattr(result, "height", 400),
                "width": 400, # getattr(result, "width", 600),
                "action": "render",
                "value": ""
            }
            print("data:", data)
            update_status_json(data)

            html_dir = os.path.join(".", "cache", "html")
            os.makedirs(html_dir, exist_ok=True)
            timestamp = int(time.time())
            html_path = os.path.join(html_dir, f"{timestamp}.html")
            html_content = ""
            if isinstance(result, list) and result:
                html_content = result[0].get("html", "")
            elif isinstance(result, dict) and "html" in result:
                html_content = result["html"]
            elif hasattr(result, "html"):
                html_content = result.html
            with open(html_path, "w", encoding="utf-8") as f:
                f.write(html_content)
            print(f"HTMLå·²ä¿å­˜åˆ°: {html_path}")

            if int(os.environ.get("DELETE_IMAGE_AFTER_PROCESS", 0)) == 1 and USE_IMAGES == 1:
                for img_path in image_files:
                    try:
                        os.remove(img_path)
                    except Exception as e:
                        print(f"åˆ é™¤å›¾ç‰‡å¤±è´¥: {img_path}, {e}")

            if os.path.exists(AUDIO_TXT_PATH):
                try:
                    with open(AUDIO_TXT_PATH, "w", encoding="utf-8") as f:
                        f.write("")
                except Exception as e:
                    print(f"åˆ é™¤audio.txtå¤±è´¥: {AUDIO_TXT_PATH}, {e}")
        except Exception as e:
            t4 = time.time()
            print(f"ä¸»è¯·æ±‚å¼‚å¸¸: {e}")
            print(f"ä¸»è¯·æ±‚è€—æ—¶: {t4-t3:.2f}ç§’")
            traceback.print_exc()

        brain_loop = int(os.environ.get("BRAIN_LOOP", 0))
        if brain_loop == 1:
            time.sleep(AI_TIME_INTERVAL)
            continue
        else:
            break

if __name__ == "__main__":
    periodic_ai_task()
