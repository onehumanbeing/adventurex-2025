from pydantic import BaseModel
from openai import OpenAI
import traceback
import os
import time
import base64
import json
import glob
import uuid
from datetime import datetime
from generate_audio import t2a_minimax

from local_tracer import get_tracer

tracer = get_tracer("./cache/logs")

HOST_URL = "https://helped-monthly-alpaca.ngrok-free.app"

class HtmlView(BaseModel):
    height: int
    width: int
    html: str
    danmu_text: str

class isFinished(BaseModel):
    result: bool

def call_openai_api(messages, thread_id=None, response_format=HtmlView, model="gpt-4o-mini", max_tokens=2000):
    """
    è°ƒç”¨OpenAI APIè¿›è¡Œå›¾åƒå’Œè¯­éŸ³åˆ†æ
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        thread_id: çº¿ç¨‹ID
        response_format: è¿”å›æ ¼å¼
        model: ä½¿ç”¨çš„æ¨¡å‹
        max_tokens: æœ€å¤§tokenæ•°ï¼Œé»˜è®¤2000
    Returns:
        (response, err): å“åº”å¯¹è±¡å’Œé”™è¯¯ï¼ˆå¦‚æœ‰ï¼‰
    """
    if thread_id is None:
        thread_id = f"local_thread_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
    
    client = OpenAI(timeout=30.0)
    try:
        if response_format == "text":
            completion = client.chat.completions.create(
                model=model,
                messages=messages,
                max_tokens=max_tokens
            )
            response = completion.choices[0].message.content
        else:
            completion = client.beta.chat.completions.parse(
                model=model,
                messages=messages,
                response_format=response_format,
                max_tokens=max_tokens
            )
            response = completion.choices[0].message.parsed
        tracer.log_brain_conversation(
            thread_id=thread_id,
            messages=messages,
            result=response
        )
        return response, None
    except Exception as e:
        print(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
        if response_format == "text":
            return "", e
        return HtmlView(
            height=400,
            width=600,
            html="<div style='padding: 16px; background-color: white; border-radius: 16px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); color: #007aff;'><h2 style='font-size: 20px; font-weight: bold; margin-bottom: 8px;'>ç³»ç»Ÿæç¤º</h2><p style='margin-bottom: 16px;'>ç½‘ç»œè¿æ¥è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚</p></div>",
            danmu_text="ç½‘ç»œè¿æ¥è¶…æ—¶"
        ), e

def update_status_json(fields: dict):
    status_path = "cache/status.json"
    if os.path.exists(status_path):
        try:
            with open(status_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception as e:
            print(f"è¯»å–status.jsonå¤±è´¥: {e}")
            data = {}
    else:
        data = {}
    data.update(fields)
    print("update_status_json:", data)
    with open(status_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def reset_status():
    data = {
        "action": "render",
        "value": "",
        "voice": "https://helped-monthly-alpaca.ngrok-free.app/voice/hello.mp3",
        "timestamp": int(time.time()),
        "html": """
<div
  style="
    height: 100%;
    width: 100%;
    background: #232323;
    display: flex;
    align-items: center;
    justify-content: center;
  "
>
  <div
    style="
      background: linear-gradient(90deg, #6ec1e4 0%, #a259c6 50%, #ff6a3d 100%);
      border-radius: 60px;
      padding: 6px;
      display: flex;
      align-items: center;
      justify-content: center;
      min-width: 480px;
      min-height: 480px;
      width: 540px;
      height: 540px;
      max-width: 90vw;
      max-height: 90vh;
      box-shadow: 0 4px 32px rgba(0, 0, 0, 0.4);
    "
  >
    <div
      style="
        background: #232323;
        border-radius: 54px;
        width: 100%;
        height: 100%;
        display: flex;
        flex-direction: column;
        overflow: hidden;
      "
    >
      <div style="padding: 40px 40px 0 40px">
        <div
          id="title"
          style="
            font-size: 56px;
            font-weight: 700;
            color: #98f7b3;
            font-family: sans-serif;
            display: flex;
            align-items: center;
            justify-content: center;
            min-height: 70px;
          "
        ></div>
      </div>
      <div
        style="
          flex: 1;
          display: flex;
          flex-direction: column;
          align-items: center;
          justify-content: center;
        "
      >
        <div
          id="desc"
          style="
            font-size: 32px;
            font-weight: 700;
            color: #fff;
            font-family: sans-serif;
            text-shadow: 0 2px 8px #000;
            text-align: center;
            min-height: 80px;
            display: flex;
            align-items: center;
            justify-content: center;
          "
        ></div>
        <div
          id="sub-desc"
          style="
            font-size: 24px;
            font-weight: 600;
            color: #ccc;
            font-family: sans-serif;
            text-shadow: 0 2px 8px #000;
            text-align: center;
            min-height: 40px;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-top: 8px;
          "
        ></div>
      </div>
      <div
        style="
          display: flex;
          align-items: center;
          justify-content: space-between;
          padding: 0 32px 24px 32px;
        "
      >
        <div>
          <div style="color: #aaa; font-size: 15px; margin-bottom: 8px">
            Searched On
          </div>
          <div style="display: flex; gap: 10px">
            <a
              href="#"
              style="
                background: #232b3e;
                color: #fff;
                border-radius: 8px;
                padding: 6px 14px;
                display: inline-flex;
                align-items: center;
                font-size: 15px;
                text-decoration: none;
              "
              ><span style="margin-right: 6px">ğŸ…‘</span>Behance</a
            >
            <a
              href="#"
              style="
                background: #1a2634;
                color: #fff;
                border-radius: 8px;
                padding: 6px 14px;
                display: inline-flex;
                align-items: center;
                font-size: 15px;
                text-decoration: none;
              "
              ><span style="margin-right: 6px">in</span>Linkedin</a
            >
            <a
              href="#"
              style="
                background: #ea4c89;
                color: #fff;
                border-radius: 8px;
                padding: 6px 14px;
                display: inline-flex;
                align-items: center;
                font-size: 15px;
                text-decoration: none;
              "
              ><span style="margin-right: 6px">ğŸ€</span>Dribbble</a
            >
            <a
              href="#"
              style="
                background: #1da1f2;
                color: #fff;
                border-radius: 8px;
                padding: 6px 14px;
                display: inline-flex;
                align-items: center;
                font-size: 15px;
                text-decoration: none;
              "
              ><span style="margin-right: 6px">ğŸ¦</span>Twitter</a
            >
          </div>
        </div>
        <div style="align-self: flex-end">
          <div style="display: flex; flex-direction: column; align-items: center;">
            <div style="width: 48px; height: 48px; background: #fff; border-radius: 50%; display: flex; align-items: center; justify-content: center;">
              <span style="font-size: 32px;">ğŸ§</span>
            </div>
            <div style="color: #fff; font-size: 18px; font-weight: 700; margin-top: 4px; font-family: sans-serif;">NoNomi AI Assistant</div>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

<div style="text-align: center; margin-top: 20px;">
  <a href="https://www.xiaohongshu.com/user/profile/620103f00000000021029b87?xsec_token=YBwTrdN0RlEzIjqWDoW7NrR9KQeXLfFH4_64sZWEgYH1g=&xsec_source=app_share&xhsshare=CopyLink&appuid=620103f00000000021029b87&apptime=1753543002&share_id=4ed639c86e0a451dad4f0e5a53ea7688" target="_blank" style="background: linear-gradient(45deg, #69bff9, #b96af3, #e9685e, #f2ac3e); background-size: 400% 100%; color: white; border: none; border-radius: 8px; padding: 10px 28px; font-size: 16px; cursor: pointer; text-decoration: none; display: inline-block; animation: rainbow 4s ease-in-out infinite; transition: transform 0.25s cubic-bezier(0.4, 2, 0.6, 1), box-shadow 0.25s; box-shadow: 0 2px 8px 0 #b96af377;">Linkä¸»åˆ›</a>
</div>
      </div>
    </div>
  </div>
</div>

<script>
// æµå¼æ–‡å­—æ›´æ–°å‡½æ•°
function updateText(elementId, newText, animationType = 'fadeIn') {
  const element = document.getElementById(elementId);
  if (element) {
    // ç§»é™¤ä¹‹å‰çš„åŠ¨ç”»ç±»
    element.classList.remove('typing-cursor', 'text-glow', 'text-bounce');
    
    // æ ¹æ®åŠ¨ç”»ç±»å‹åº”ç”¨ä¸åŒçš„æ•ˆæœ
    switch(animationType) {
      case 'typewriter':
        typewriterEffect(element, newText);
        break;
      case 'bounce':
        bounceEffect(element, newText);
        break;
      case 'glow':
        glowEffect(element, newText);
        break;
      case 'fadeIn':
      default:
        fadeInEffect(element, newText);
        break;
    }
  }
}

// æ‰“å­—æœºæ•ˆæœ
function typewriterEffect(element, text) {
  element.textContent = '';
  element.classList.add('typing-cursor');
  let index = 0;
  
  function type() {
    if (index < text.length) {
      element.textContent += text.charAt(index);
      index++;
      setTimeout(type, 100);
    } else {
      element.classList.remove('typing-cursor');
    }
  }
  type();
}

// å¼¹è·³æ•ˆæœ
function bounceEffect(element, text) {
  element.style.animation = 'none';
  element.offsetHeight; // è§¦å‘é‡æ’
  element.innerHTML = text;
  element.style.animation = 'textBounce 0.8s ease-out';
}

// å‘å…‰æ•ˆæœ
function glowEffect(element, text) {
  element.innerHTML = text;
  element.classList.add('text-glow');
  setTimeout(() => {
    element.classList.remove('text-glow');
  }, 2000);
}

// æ·¡å…¥æ•ˆæœ
function fadeInEffect(element, text) {
  element.style.animation = 'none';
  element.offsetHeight; // è§¦å‘é‡æ’
  element.innerHTML = text;
  element.style.animation = 'textFadeIn 0.6s ease-out';
}

// åˆå§‹åŒ–æ–‡å­—
updateText('title', 'Hello!', 'bounce');
updateText('desc', 'æ­£åœ¨ä¸ºæ‚¨åˆ†æå½“å‰åœºæ™¯ä¿¡æ¯', 'fadeIn');
updateText('sub-desc', 'æ„å»ºAGIæ£®æ—ğŸŒ³', 'fadeIn');

// æ¨¡æ‹Ÿæµå¼æ›´æ–° - ä½¿ç”¨ä¸åŒåŠ¨ç”»æ•ˆæœ
setTimeout(() => {
  updateText('desc', 'åˆ›é€ åŠ›æ˜¯æ™ºæ…§åœ¨ç©è€', 'typewriter');
}, 3000);

setTimeout(() => {
  updateText('desc', 'ç”Ÿæ´»ä¸æ˜¯å¯»æ‰¾è‡ªå·±ï¼Œè€Œæ˜¯åˆ›é€ è‡ªå·±', 'bounce');
}, 8000);

setTimeout(() => {
  updateText('title', 'NoNomi', 'glow');
  updateText('desc', 'æ¿€å‘åˆ›é€  â€¢ ä¸°å¯Œç”Ÿæ´»', 'fadeIn');
  updateText('sub-desc', 'æ„å»ºAGIæ£®æ—ğŸŒ³', 'fadeIn');
}, 12000);
</script>
""",
        "danmu_text": "QAQ",
        "height": 400,
        "width": 600
    }
    update_status_json(data)

def periodic_ai_task():
    reset_status()
    time.sleep(2)
    print("å¼€å§‹AIä»»åŠ¡")
    AI_TIME_INTERVAL = int(os.environ.get("AI_TIME_INTERVAL", 5))
    SCREENSHOT_UPLOAD_AMOUNT = int(os.environ.get("SCREENSHOT_UPLOAD_AMOUNT", 1))
    USE_CAMERA = int(os.environ.get("USE_CAMERA", 0))
    if USE_CAMERA == 1:
        IMAGE_DIR = os.path.join(".", "cache", "camera")
    else:
        IMAGE_DIR = os.path.join(".", "cache", "screenshot")
    AUDIO_TXT_PATH = os.path.join(".", "cache", "audio", "audio.txt")
    PROMPT_PATH = os.path.join(".", "prompt.txt")
    PROMPT_IMAGE_PATH = os.path.join(".", "prompt_image.txt")
    PROMPT_FINISHED_PATH = os.path.join(".", "prompt_finished.txt")

    while True:
        current_timestamp = int(time.time())

        # æ­¥éª¤1: æ£€æŸ¥éŸ³é¢‘å†…å®¹å¹¶åˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘AIå›å¤
        audio_content = None
        if os.path.exists(AUDIO_TXT_PATH):
            try:
                with open(AUDIO_TXT_PATH, "r", encoding="utf-8") as f:
                    audio_content = f.read().strip()
            except Exception as e:
                print(f"è¯»å–audio.txtå¤±è´¥: {e}")

        if not audio_content:
            print("æœªæ£€æµ‹åˆ°éŸ³é¢‘å†…å®¹ï¼Œ5ç§’åé‡è¯•ã€‚")
            time.sleep(5)
            continue

        # ä½¿ç”¨isFinishedæ¨¡å‹åˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘AIå›å¤
        try:
            with open(PROMPT_FINISHED_PATH, "r", encoding="utf-8") as f:
                finished_prompt = f.read().strip()
        except Exception as e:
            print(f"è¯»å–prompt_finished.txtå¤±è´¥: {e}")
            finished_prompt = ""

        if finished_prompt:
            finished_messages = [
                {"role": "system", "content": finished_prompt},
                {"role": "user", "content": audio_content}
            ]
            print("å¼€å§‹åˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘AIå›å¤", audio_content)
            try:
                finished_result, finished_err = call_openai_api(
                    finished_messages,
                    response_format=isFinished,
                    model="gpt-4o-mini",
                    max_tokens=50  # isFinishedåªéœ€è¦è¿”å›å¸ƒå°”å€¼ï¼Œ50ä¸ªtokenè¶³å¤Ÿ
                )
                if finished_err is not None:
                    print("åˆ¤æ–­è¯·æ±‚å¼‚å¸¸:", finished_err)
                    time.sleep(AI_TIME_INTERVAL)
                    continue
                
                if not finished_result.result:
                    print("âŒéŸ³é¢‘å†…å®¹ä¸éœ€è¦è§¦å‘AIå›å¤ï¼Œè·³è¿‡æœ¬æ¬¡å¤„ç†")
                    # æ£€æŸ¥å¦‚æœaudio.txtçš„è¡Œæ•°å¤§äº10è¡Œï¼Œåˆ™æ¸…ç©º
                    if os.path.exists(AUDIO_TXT_PATH):
                        with open(AUDIO_TXT_PATH, "r", encoding="utf-8") as f:
                            lines = f.readlines()
                            if len(lines) > 10:
                                with open(AUDIO_TXT_PATH, "w", encoding="utf-8") as f:
                                    f.write("")
                    time.sleep(3)
                    continue
                print("âœ…éŸ³é¢‘å†…å®¹éœ€è¦è§¦å‘AIå›å¤ï¼Œç»§ç»­å¤„ç†")
                # æ¸…ç©ºéŸ³é¢‘æ–‡ä»¶
                try:
                    with open(AUDIO_TXT_PATH, "w", encoding="utf-8") as f:
                        f.write("")
                except Exception as e:
                    print(f"æ¸…ç©ºaudio.txtå¤±è´¥: {e}")
                time.sleep(AI_TIME_INTERVAL)
                update_status_json({
                    "action": "pending",
                    "voice": "https://helped-monthly-alpaca.ngrok-free.app/voice/pending.mp3",
                    "timestamp": int(time.time()),
                })
            except Exception as e:
                print(f"åˆ¤æ–­è¯·æ±‚å¼‚å¸¸: {e}")
                time.sleep(AI_TIME_INTERVAL)
                continue
        else:
            print("ç¼ºå°‘finished promptï¼Œè·³è¿‡åˆ¤æ–­æ­¥éª¤")

        # æ­¥éª¤2: å¦‚æœæœ‰å›¾ç‰‡ï¼Œåˆ™è·å–å›¾ç‰‡æ•°é‡å¹¶ä¸”è§£æ
        image_files = glob.glob(os.path.join(IMAGE_DIR, "*"))
        image_files = [f for f in image_files if os.path.isfile(f)]
        image_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
        latest_images = image_files[:SCREENSHOT_UPLOAD_AMOUNT]

        image_messages = []
        if len(image_files) > 0:
            print(f"æ£€æµ‹åˆ° {len(image_files)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹å¤„ç†")
            for img_path in latest_images:
                try:
                    with open(img_path, "rb") as f:
                        b64_img = base64.b64encode(f.read()).decode("utf-8")
                    image_messages.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}
                    })
                except Exception as e:
                    print(f"è¯»å–å›¾ç‰‡å¤±è´¥: {img_path}, {e}")
        else:
            print("æœªæ£€æµ‹åˆ°å›¾ç‰‡ï¼Œè·³è¿‡å›¾ç‰‡å¤„ç†æ­¥éª¤")

        try:
            with open(PROMPT_PATH, "r", encoding="utf-8") as f:
                system_prompt = f.read().strip()
        except Exception as e:
            print(f"è¯»å–prompt.txtå¤±è´¥: {e}")
            system_prompt = ""

        try:
            with open(PROMPT_IMAGE_PATH, "r", encoding="utf-8") as f:
                prompt_image = f.read().strip()
        except Exception as e:
            print(f"è¯»å–prompt_image.txtå¤±è´¥: {e}")
            prompt_image = ""

        screen_description = None
        image_analysis_error = False
        t1 = time.time()
        if prompt_image and image_messages:
            image_analysis_messages = [
                {"role": "system", "content": prompt_image},
                {"role": "user", "content": image_messages},
                {"role": "user", "content": f"audio: {audio_content}"}
            ]
            print("å¼€å§‹å›¾ç‰‡ç»†èŠ‚åˆ†æè¯·æ±‚", latest_images, audio_content)
            try:
                response, err = call_openai_api(
                    image_analysis_messages,
                    response_format="text",
                    max_tokens=500  # å›¾ç‰‡åˆ†ææ–‡æœ¬ï¼Œé€šå¸¸500ä¸ªtokenè¶³å¤Ÿæè¿°å›¾ç‰‡
                )
                t2 = time.time()
                print("âœ… å›¾ç‰‡ç»†èŠ‚åˆ†æç»“æœ:", response)
                print(f"å›¾ç‰‡ç»†èŠ‚åˆ†æè€—æ—¶: {t2-t1:.2f}ç§’")
                if err is not None:
                    print("å›¾ç‰‡ç»†èŠ‚åˆ†æè¯·æ±‚å¼‚å¸¸:", err)
                    image_analysis_error = True
                elif isinstance(response, str):
                    screen_description = response
                elif hasattr(response, "content"):
                    screen_description = getattr(response, "content", None)
                else:
                    screen_description = str(response)
            except Exception as e:
                t2 = time.time()
                print(f"å›¾ç‰‡ç»†èŠ‚åˆ†æè¯·æ±‚å¼‚å¸¸: {e}")
                print(f"å›¾ç‰‡ç»†èŠ‚åˆ†æè€—æ—¶: {t2-t1:.2f}ç§’")
                image_analysis_error = True
        else:
            t2 = time.time()
            print("æœªèƒ½è¿›è¡Œå›¾ç‰‡ç»†èŠ‚åˆ†æï¼ˆç¼ºå°‘prompt_imageæˆ–å›¾ç‰‡ï¼‰")
            image_analysis_error = True

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ éŸ³é¢‘å†…å®¹ï¼ˆæ ¼å¼åŒ–ä¸ºaudio:å‰ç¼€ï¼‰
        if audio_content:
            messages.append({"role": "user", "content": f"audio: {audio_content}"})
        
        # å¦‚æœæœ‰å›¾ç‰‡åˆ†æç»“æœï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
        if not image_analysis_error and screen_description:
            messages.append({"role": "user", "content": f"screen_description: {screen_description}"})
        else:
            print("å›¾ç‰‡åˆ†æå¤±è´¥ï¼Œä¸ä¼ é€’å›¾ç‰‡ä¿¡æ¯ç»™ä¸»è¯·æ±‚ã€‚")
        
        if not messages:
            print("æ²¡æœ‰å¯ç”¨çš„è¾“å…¥ï¼Œè·³è¿‡æœ¬æ¬¡è°ƒç”¨ã€‚")
            time.sleep(AI_TIME_INTERVAL)
            continue
        print("æœ¬æ¬¡å¤„ç†çš„å›¾ç‰‡è·¯å¾„:", latest_images)
        t3 = time.time()
        try:
            # ç”ŸæˆåŸºäºå½“å‰æ—¶é—´çš„thread_id
            current_thread_id = f"local_brain_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
            result, err = call_openai_api(messages, thread_id=current_thread_id, max_tokens=2000)  # HtmlRenderä¸»è¯·æ±‚ï¼Œéœ€è¦æ›´å¤štokenæ¥ç”ŸæˆHTMLå’Œå¼¹å¹•
            t4 = time.time()
            print(f"âœ… ä¸»è¯·æ±‚è€—æ—¶: {t4-t3:.2f}ç§’")
            print("âœ… AI HTMLç»“æœ:", result)
            print(f"Thread ID: {current_thread_id}")
            # tracer.log_brain_conversation(
            #     thread_id=current_thread_id,
            #     messages=messages,
            #     result=result,
            #     image_paths=latest_images,
            #     audio_content=f"audio: {audio_content}" if audio_content else None
            # )
            if hasattr(result, "danmu_text") and result.danmu_text != "":
                audio = t2a_minimax(result.danmu_text)
                with open(f"cache/voice/{current_timestamp}.mp3", "wb") as f:
                    f.write(audio)
                route = f"/voice/{current_timestamp}.mp3"
                print("minimaxç»“æœ:", route)
            else:
                route = ""

            data = {
                "voice": "" if route == "" else f"{HOST_URL}{route}",
                "timestamp": int(time.time()),
                "html": getattr(result, "html", ""),
                "danmu_text": getattr(result, "danmu_text", ""),
                "height": getattr(result, "height", 400),
                "width": getattr(result, "width", 600),
                "action": "render",
                "value": ""
            }
            print("data:", data)
            update_status_json(data)

            html_dir = os.path.join(".", "cache", "html")
            os.makedirs(html_dir, exist_ok=True)
            timestamp = int(time.time())
            html_path = os.path.join(html_dir, f"{timestamp}.html")
            html_content = ""
            if isinstance(result, list) and result:
                html_content = result[0].get("html", "")
            elif isinstance(result, dict) and "html" in result:
                html_content = result["html"]
            elif hasattr(result, "html"):
                html_content = result.html
            with open(html_path, "w", encoding="utf-8") as f:
                f.write(html_content)
            print(f"HTMLå·²ä¿å­˜åˆ°: {html_path}")

            if int(os.environ.get("DELETE_IMAGE_AFTER_PROCESS", 0)) == 1:
                for img_path in image_files:
                    try:
                        os.remove(img_path)
                    except Exception as e:
                        print(f"åˆ é™¤å›¾ç‰‡å¤±è´¥: {img_path}, {e}")

            if os.path.exists(AUDIO_TXT_PATH):
                try:
                    with open(AUDIO_TXT_PATH, "w", encoding="utf-8") as f:
                        f.write("")
                except Exception as e:
                    print(f"åˆ é™¤audio.txtå¤±è´¥: {AUDIO_TXT_PATH}, {e}")
        except Exception as e:
            t4 = time.time()
            print(f"ä¸»è¯·æ±‚å¼‚å¸¸: {e}")
            print(f"ä¸»è¯·æ±‚è€—æ—¶: {t4-t3:.2f}ç§’")
            traceback.print_exc()

        brain_loop = int(os.environ.get("BRAIN_LOOP", 0))
        if brain_loop == 1:
            time.sleep(AI_TIME_INTERVAL)
            continue
        else:
            break

if __name__ == "__main__":
    periodic_ai_task()
